{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os, sys\n",
    "import imutils\n",
    "from scipy import spatial as sp\n",
    "import collections\n",
    "#import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of Lists FOR KEEPING THE CENTROIDS OF THE OBJECTS IN TRACK\n",
    "\n",
    "personwise_track = []*0\n",
    "\n",
    "NMS_THRESHOLD=0.3\n",
    "MIN_CONFIDENCE=0.2\n",
    "\n",
    "labelsPath = \"/home/arima/Downloads/yolo_pres/coco.names\"\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "weights_path = \"/home/arima/Downloads/yolo_pres/yolov4-tiny.weights\"\n",
    "config_path = \"/home/arima/Downloads/yolo_pres/yolov4-tiny.cfg\"\n",
    "\n",
    "model = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pedestrian_detection(image, model, layer_name, personidz=0):\n",
    "\t(H, W) = image.shape[:2]\n",
    "\tresults = []\n",
    "\n",
    "\n",
    "\tblob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416),\n",
    "\t\tswapRB=True, crop=False)\n",
    "\tmodel.setInput(blob)\n",
    "\tlayerOutputs = model.forward(layer_name)\n",
    "\n",
    "\tboxes = []\n",
    "\tcentroids = []\n",
    "\tconfidences = []\n",
    "\tcount = 0\n",
    "\n",
    "\tfor output in layerOutputs:\n",
    "\t\tfor detection in output:\n",
    "\t\t\tscores = detection[5:]\n",
    "\t\t\t#print(detection)\n",
    "\t\t\t#print(scores)\n",
    "\t\t\tclassID = np.argmax(scores)\n",
    "\t\t\tconfidence = scores[classID]\n",
    "\n",
    "\t\t\tif classID == personidz and confidence > MIN_CONFIDENCE:\n",
    "\t\t\t\t\n",
    "\t\t\t\t\n",
    "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
    "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t\tx = int(centerX - (width / 2))\n",
    "\t\t\t\ty = int(centerY - (height / 2))\n",
    "\n",
    "\t\t\t\t## Arima : Color codes can be created from Centroids and Area. Centroids are relatively easier. \n",
    "\t\t\t\t# If centroid goes on going towards up and vanish, then its going away, \n",
    "\t\t\t\t# if the centroids are going towards down, then its coming towards.\n",
    "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\t\tcentroids.append((centerX, centerY))\n",
    "\t\t\t\tconfidences.append(float(confidence))\n",
    "\t# apply non-maxima suppression to suppress weak, overlapping\n",
    "\t# bounding boxes\n",
    "\tidzs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONFIDENCE, NMS_THRESHOLD)\n",
    "\t# ensure at least one detection exists\n",
    "\tafter_NMS = []*0\n",
    "\tif len(idzs) > 0:\n",
    "\t\t# loop over the indexes we are keeping\n",
    "\t\tfor i in idzs.flatten():\n",
    "\t\t\tcount+=1\n",
    "\n",
    "\t\t\t# extract the bounding box coordinates\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\t\t\t# update our results list to consist of the person\n",
    "\t\t\t# prediction probability, bounding box coordinates,\n",
    "\t\t\t# and the centroid\n",
    "\t\t\tafter_NMS.append(centroids[i])\n",
    "\t\t\t#print(x, y, x + w, y + h)\n",
    "\t\t\tres = (confidences[i], (x, y, x + w, y + h), centroids[i], count)\n",
    "\t\t\tresults.append(list(res))\n",
    "\t# return the list of results\n",
    "\treturn results, len(after_NMS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "model.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "'''\n",
    "\n",
    "layer_name = model.getLayerNames()\n",
    "layer_name = [layer_name[i - 1] for i in model.getUnconnectedOutLayers()]\n",
    "\n",
    "#cap = cv2.VideoCapture(\"/home/arima/Downloads/yolo_pres/video_2022-04-07_13-07-25.mp4\")\n",
    "#cap = cv2.VideoCapture(\"/home/arima/Downloads/yolo_pres/video_2022-04-07_13-09-50.mp4\")\n",
    "#cap = cv2.VideoCapture(\"/home/arima/Downloads/yolo_pres/video_2022-04-07_13-09-52.mp4\")\n",
    "cap = cv2.VideoCapture(\"/home/arima/Downloads/yolo_pres/video_2022-04-07_13-09-55.mp4\")\n",
    "#cap = cv2.VideoCapture(\"/home/arima/Downloads/yolo_pres/video_2022-04-07_13-11-48.mp4\")\n",
    "#cap = cv2.VideoCapture(\"/home/arima/Downloads/yolo_pres/video_2022-04-07_13-07-25.mp4\")\n",
    "#cap = cv2.VideoCapture(\"/home/arima/Downloads/yolo_pres/mixkit-people-in-the-subway-hall-in-tokyo-4454.mp4\")\n",
    "\n",
    "writer = None\n",
    "\n",
    "f_c = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Bounding Boxes in the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\t(grabbed, image) = cap.read()\n",
    "\n",
    "\tif not grabbed:\n",
    "\t\tbreak\n",
    "\timage = imutils.resize(image, width=700)\n",
    "\th, w = image.shape[0], image.shape[1]\n",
    "\t#print(h,w)\n",
    "\tresults, numbers = pedestrian_detection(image, model, layer_name,\n",
    "\t\tpersonidz=LABELS.index(\"person\"))\n",
    "\t\n",
    "\tcentroids_of_this_frame = []*0\n",
    "\tareas_of_bounding_boxes_of_this_frame = {}\n",
    "\tfor res in results:\n",
    "\t\tcentroids_of_this_frame.append(res[2])\n",
    "\t\tareas_of_bounding_boxes_of_this_frame.update({res[2]:res[1][2]*res[1][3]})\n",
    "\t\n",
    "\tabbtf = sorted(areas_of_bounding_boxes_of_this_frame.items(), key = lambda kv:(kv[1], kv[0]))\n",
    "\n",
    "\tneigh = {\"orange\":[], \"green\":[], \"red\":[]}\n",
    "\tvote = {}\n",
    "\tvote_status = {}\n",
    "\t\n",
    "\tfor i in centroids_of_this_frame:\n",
    "\t\tvote.update({i:[]*0})\n",
    "\n",
    "\tfor i in range(len(centroids_of_this_frame)):\n",
    "\t\tfor j in range(i, len(centroids_of_this_frame)):\n",
    "\t\t\tif(sp.distance.euclidean(centroids_of_this_frame[j],centroids_of_this_frame[i]) >= 25.0 and sp.distance.euclidean(centroids_of_this_frame[j],centroids_of_this_frame[i]) <= 50.0):\n",
    "\t\t\t\tneigh[\"orange\"].append((centroids_of_this_frame[i], centroids_of_this_frame[j], sp.distance.euclidean(centroids_of_this_frame[j],centroids_of_this_frame[i])))\n",
    "\t\t\t\tvote[centroids_of_this_frame[i]].append(\"orange\")\n",
    "\t\t\t\tvote[centroids_of_this_frame[j]].append(\"orange\")\n",
    "\t\t\t\n",
    "\t\t\telif(sp.distance.euclidean(centroids_of_this_frame[j],centroids_of_this_frame[i]) != 0.0 and sp.distance.euclidean(centroids_of_this_frame[j],centroids_of_this_frame[i]) <= 25.0):\n",
    "\t\t\t\tneigh[\"red\"].append((centroids_of_this_frame[i], centroids_of_this_frame[j], sp.distance.euclidean(centroids_of_this_frame[j],centroids_of_this_frame[i])))\n",
    "\t\t\t\tvote[centroids_of_this_frame[i]].append(\"red\")\n",
    "\t\t\t\tvote[centroids_of_this_frame[j]].append(\"red\")\n",
    "\t\t\t\t\n",
    "\t\t\telif(sp.distance.euclidean(centroids_of_this_frame[j],centroids_of_this_frame[i]) >= 50.0):\n",
    "\t\t\t\tneigh[\"green\"].append((centroids_of_this_frame[i], centroids_of_this_frame[j], sp.distance.euclidean(centroids_of_this_frame[j],centroids_of_this_frame[i])))\n",
    "\t\t\t\tvote[centroids_of_this_frame[i]].append(\"green\")\n",
    "\t\t\t\tvote[centroids_of_this_frame[j]].append(\"green\")\n",
    "\t\t\t\t\n",
    "\t# No. Of Frames\n",
    "\tcv2.putText(image, str(numbers), (20,20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 4)\n",
    "\tcv2.imshow(\"Detection\",image)\n",
    "\t\n",
    "\tkey = cv2.waitKey(1)\n",
    "\tif key == 27:\n",
    "\t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Person Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\t(grabbed, image) = cap.read()\n",
    "\n",
    "\tif not grabbed:\n",
    "\t\tbreak\n",
    "\timage = imutils.resize(image, width=700)\n",
    "\th, w = image.shape[0], image.shape[1]\n",
    "\t#print(h,w)\n",
    "\tresults, numbers = pedestrian_detection(image, model, layer_name,\n",
    "\t\tpersonidz=LABELS.index(\"person\"))\n",
    "\t\n",
    "\tcentroids_of_this_frame = []*0\n",
    "\tareas_of_bounding_boxes_of_this_frame = {}\n",
    "\tfor res in results:\n",
    "\t\tcentroids_of_this_frame.append(res[2])\n",
    "\t\tareas_of_bounding_boxes_of_this_frame.update({res[2]:res[1][2]*res[1][3]})\n",
    "\t\n",
    "\tabbtf = sorted(areas_of_bounding_boxes_of_this_frame.items(), key = lambda kv:(kv[1], kv[0]))\n",
    "\n",
    "\tneigh = {\"orange\":[], \"green\":[], \"red\":[]}\n",
    "\tvote = {}\n",
    "\tvote_status = {}\n",
    "\t\n",
    "\tfor res in results:\n",
    "\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 165, 255), 2)\n",
    "\t\tcv2.putText(image, str(res[-1]), (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Approach\n",
    "\t\t\n",
    "\t# No. Of Frames\n",
    "\tcv2.putText(image, str(numbers), (20,20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 4)\n",
    "\n",
    "\tcv2.imshow(\"Detection\",image)\n",
    "\tf_c += 1\n",
    "\t\n",
    "\tkey = cv2.waitKey(1)\n",
    "\tif key == 27:\n",
    "\t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaching Departing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\t(grabbed, image) = cap.read()\n",
    "\n",
    "\tif not grabbed:\n",
    "\t\tbreak\n",
    "\timage = imutils.resize(image, width=700)\n",
    "\th, w = image.shape[0], image.shape[1]\n",
    "\t#print(h,w)\n",
    "\tresults, numbers = pedestrian_detection(image, model, layer_name,\n",
    "\t\tpersonidz=LABELS.index(\"person\"))\n",
    "\t\n",
    "\tcentroids_of_this_frame = []*0\n",
    "\tareas_of_bounding_boxes_of_this_frame = {}\n",
    "\tfor res in results:\n",
    "\t\tcentroids_of_this_frame.append(res[2])\n",
    "\t\tareas_of_bounding_boxes_of_this_frame.update({res[2]:res[1][2]*res[1][3]})\n",
    "\t\n",
    "\tabbtf = sorted(areas_of_bounding_boxes_of_this_frame.items(), key = lambda kv:(kv[1], kv[0]))\n",
    "\n",
    "\n",
    "\tfor res in results:\n",
    "\t\tif (res[1][2]*res[1][3])<100:\n",
    "\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 165, 255), 2)\n",
    "\t\t\t\tcv2.putText(image, str(res[-1])+\"D\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Depart\n",
    "\t\t\t#coming towards : script kiddie version\n",
    "\t\telse:\n",
    "\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 165, 255), 2)\n",
    "\t\t\t\tcv2.putText(image, str(res[-1])+\"A\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Approach\n",
    "\t\t\n",
    "\t# No. Of Frames\n",
    "\tcv2.putText(image, str(numbers), (20,20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 4)\n",
    "\n",
    "\tcv2.imshow(\"Detection\",image)\n",
    "\tf_c += 1\n",
    "\t\n",
    "\tkey = cv2.waitKey(1)\n",
    "\tif key == 27:\n",
    "\t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\t(grabbed, image) = cap.read()\n",
    "\n",
    "\tif not grabbed:\n",
    "\t\tbreak\n",
    "\timage = imutils.resize(image, width=700)\n",
    "\th, w = image.shape[0], image.shape[1]\n",
    "\t#print(h,w)\n",
    "\tresults, numbers = pedestrian_detection(image, model, layer_name,\n",
    "\t\tpersonidz=LABELS.index(\"person\"))\n",
    "\t\n",
    "\tcentroids_of_this_frame = []*0\n",
    "\tareas_of_bounding_boxes_of_this_frame = {}\n",
    "\tfor res in results:\n",
    "\t\tcentroids_of_this_frame.append(res[2])\n",
    "\t\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\tx, y\t\t\t\t\t\tx+w, y\t\t\t\t\tx, y+h\t\t\t\t\tx+w, y+h\n",
    "\t\tareas_of_bounding_boxes_of_this_frame.update({res[2]:((res[1][0], res[1][1]), (res[1][3], res[1][0]), (res[1][0], res[1][3]), (res[1][2], res[1][3]))})\n",
    "\t\n",
    "\tabbtf = sorted(areas_of_bounding_boxes_of_this_frame.items(), key = lambda kv:(kv[1], kv[0]))\n",
    "\n",
    "\tprint(abbtf)\n",
    "\t\n",
    "\t'''\n",
    "\tfor i in range(len(abbtf)):\n",
    "\t\tif(i<=len(abbtf)):\n",
    "\t\t\tabbtf[i] = list(abbtf[i])\n",
    "\t\t\tabbtf[i].append(\"\")\n",
    "\n",
    "\tinput()\n",
    "\t'''\n",
    "\t#neigh = {\"orange\":[], \"green\":[], \"red\":[]}\n",
    "\tneigh = {}\n",
    "\tvote = {}\n",
    "\tvote_status = {}\n",
    "\tfor i in centroids_of_this_frame:\n",
    "\t\tvote.update({i:[]*0})\n",
    "\t\n",
    "\t# Code for Area of overlap\n",
    "\t# Take any two boxes, calculate euclidean distance between (X,Y) (x+w,y)\n",
    "\n",
    "\tfor i in range(len(centroids_of_this_frame)-1):\n",
    "\t\t#Box_prime = areas_of_bounding_boxes_of_this_frame[centroids_of_this_frame[i]]\n",
    "\t\tBox_prime = centroids_of_this_frame[i]\n",
    "\t\t#neigh.update({centroids_of_this_frame[i]:[]*0})\n",
    "\t\tprint(\"Box Prime  :\", Box_prime)\n",
    "\t\tfor j in range(1, len(centroids_of_this_frame)):\n",
    "\t\t\t#Box_sec = areas_of_bounding_boxes_of_this_frame[centroids_of_this_frame[j]]\n",
    "\t\t\tBox_sec = centroids_of_this_frame[j]\n",
    "\t\t\tprint(\"Box Secondary  :\", Box_sec)\n",
    "\t\t\n",
    "\t\t\t#if(sp.distance.euclidean(Box_prime[1], Box_sec[0])<=0):\n",
    "\t\t\tif(sp.distance.euclidean(Box_prime[1], Box_sec[0])<=50.0):\n",
    "\n",
    "\t\t\t\t#neigh[centroids_of_this_frame[i]].append([centroids_of_this_frame[j], \"red\"])\n",
    "\t\t\t\tneigh[centroids_of_this_frame[i]] = neigh[centroids_of_this_frame[j]] = \"red\"\n",
    "\n",
    "\t\t\t#elif(0 < sp.distance.euclidean(Box_prime[1], Box_sec[0]) <= 25):\n",
    "\t\t\telif(50.0 < sp.distance.euclidean(Box_prime[1], Box_sec[0])<=300.0):\n",
    "\n",
    "\t\t\t\t#neigh[centroids_of_this_frame[i]].append([centroids_of_this_frame[j], \"orange\"])\n",
    "\t\t\t\tneigh[centroids_of_this_frame[i]] = neigh[centroids_of_this_frame[j]] = \"orange\"\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\t\n",
    "\t\t\t\t#neigh[centroids_of_this_frame[i]].append([centroids_of_this_frame[j], \"green\"])\n",
    "\t\t\t\tneigh[centroids_of_this_frame[i]]  = neigh[centroids_of_this_frame[j]] = \"green\"\n",
    "\t\n",
    "\tfor res in results:\n",
    "\t\tprint(res[2])\n",
    "\t\tif (neigh[res[2]]):\n",
    "\t\t\t#print(neigh[res[2]][0][1])\n",
    "\t\t\tif(neigh[res[2]] == \"orange\"):\n",
    "\t\t\t\t# going away : script kiddie version\n",
    "\t\t\t\t#if (h*w) - ((res[1][2]-res[1][0])*(res[1][3]res[1][1]))<100:\n",
    "\t\t\t\tif ((res[1][2]-res[1][0])*(res[1][3]-res[1][1]))<100:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 165, 255), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"D\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Depart\n",
    "\t\t\t\t#coming towards : script kiddie version\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 165, 255), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"A\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Approach\n",
    "\t\t\t\n",
    "\t\t\telif(neigh[res[2]] == \"red\"):\n",
    "\t\t\t\t# going away : script kiddie version\n",
    "\t\t\t\t#if (h*w) - ((res[1][2]-res[1][0])*(res[1][3]res[1][1]))<100:\n",
    "\t\t\t\tif ((res[1][2]-res[1][0])*(res[1][3]-res[1][1]))<100:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 0, 255), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"D\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Depart\n",
    "\t\t\t\t#coming towards : script kiddie version\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 0, 255), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"A\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Approach\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\t# going away : script kiddie version\n",
    "\t\t\t\t#if (h*w) - ((res[1][2]-res[1][0])*(res[1][3]res[1][1]))<100:\n",
    "\t\t\t\tif ((res[1][2]-res[1][0])*(res[1][3]-res[1][1]))<100:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 255, 0), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"D\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Depart\n",
    "\t\t\t\t#coming towards : script kiddie version\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 255, 0), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"A\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Approach\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\n",
    "\t\t\tif ((res[1][2]-res[1][0])*(res[1][3]-res[1][1]))<100:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 255, 0), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"D\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Depart\n",
    "\t\t\t\t#coming towards : script kiddie version\n",
    "\t\t\telse:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 255, 0), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"A\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Approach\n",
    "\t\t\t\t\t \n",
    "\t\t\n",
    "\t# No. Of Frames\n",
    "\tcv2.putText(image, str(numbers), (20,20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 4)\n",
    "\n",
    "\tcv2.imshow(\"Detection\",image)\n",
    "\tf_c += 1\n",
    "\tif(f_c%10==0):\n",
    "\t\tcv2.imwrite(\"/home/arima/Downloads/yolo_pres/frames_op_with_cnt/frame_{}.jpg\".format(f_c), image)\n",
    "\n",
    "\tkey = cv2.waitKey(1)\n",
    "\tif key == 27:\n",
    "\t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedestrian Tracking\n",
    "\n",
    "Direction of Movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((82, 267), ((36, 150), (384, 36), (36, 384), (127, 384))), ((367, 170), ((358, 150), (189, 358), (358, 189), (375, 189))), ((682, 278), ((665, 159), (396, 665), (665, 396), (698, 396)))]\n",
      "Box Prime  : (82, 267)\n",
      "Box Secondary  : (682, 278)\n",
      "Box Secondary  : (367, 170)\n",
      "Box Prime  : (682, 278)\n",
      "Box Secondary  : (682, 278)\n",
      "Box Secondary  : (367, 170)\n",
      "(82, 267)\n",
      "(682, 278)\n",
      "(367, 170)\n",
      "[((84, 266), ((39, 151), (381, 39), (39, 381), (129, 381))), ((368, 170), ((359, 150), (190, 359), (359, 190), (376, 190)))]\n",
      "Box Prime  : (84, 266)\n",
      "Box Secondary  : (368, 170)\n",
      "(84, 266)\n",
      "(368, 170)\n",
      "[((87, 267), ((41, 152), (381, 41), (41, 381), (133, 381))), ((368, 170), ((359, 150), (189, 359), (359, 189), (376, 189)))]\n",
      "Box Prime  : (87, 267)\n",
      "Box Secondary  : (368, 170)\n",
      "(87, 267)\n",
      "(368, 170)\n",
      "[((89, 267), ((40, 153), (380, 40), (40, 380), (137, 380))), ((368, 170), ((359, 150), (189, 359), (359, 189), (376, 189)))]\n",
      "Box Prime  : (89, 267)\n",
      "Box Secondary  : (368, 170)\n",
      "(89, 267)\n",
      "(368, 170)\n",
      "[((93, 267), ((45, 153), (381, 45), (45, 381), (140, 381))), ((368, 170), ((359, 150), (189, 359), (359, 189), (376, 189)))]\n",
      "Box Prime  : (93, 267)\n",
      "Box Secondary  : (368, 170)\n",
      "(93, 267)\n",
      "(368, 170)\n",
      "[((101, 268), ((58, 154), (382, 58), (58, 382), (144, 382))), ((368, 169), ((359, 150), (188, 359), (359, 188), (376, 188)))]\n",
      "Box Prime  : (101, 268)\n",
      "Box Secondary  : (368, 169)\n",
      "(101, 268)\n",
      "(368, 169)\n",
      "[((103, 267), ((59, 152), (381, 59), (59, 381), (147, 381)))]\n",
      "(103, 267)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(103, 267)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mprint\u001b[39m(res[\u001b[39m2\u001b[39m])\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m ctr\u001b[39m%\u001b[39m\u001b[39m10\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:cv2\u001b[39m.\u001b[39mcircle(bi, res[\u001b[39m2\u001b[39m], \u001b[39m1\u001b[39m, (\u001b[39m255\u001b[39m, \u001b[39m34\u001b[39m, \u001b[39m38\u001b[39m), \u001b[39m3\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[39mif\u001b[39;00m (neigh[res[\u001b[39m2\u001b[39;49m]]):\n\u001b[1;32m     86\u001b[0m \t\u001b[39m#print(neigh[res[2]][0][1])\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \t\u001b[39mif\u001b[39;00m(neigh[res[\u001b[39m2\u001b[39m]] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39morange\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     88\u001b[0m \t\t\u001b[39m# going away : script kiddie version\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \t\t\u001b[39m#if (h*w) - ((res[1][2]-res[1][0])*(res[1][3]res[1][1]))<100:\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \t\t\u001b[39mif\u001b[39;00m ((res[\u001b[39m1\u001b[39m][\u001b[39m2\u001b[39m]\u001b[39m-\u001b[39mres[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m])\u001b[39m*\u001b[39m(res[\u001b[39m1\u001b[39m][\u001b[39m3\u001b[39m]\u001b[39m-\u001b[39mres[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]))\u001b[39m<\u001b[39m\u001b[39m100\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: (103, 267)"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "\t(grabbed, image) = cap.read()\n",
    "\n",
    "\tif not grabbed:\n",
    "\t\tbreak\n",
    "\timage = imutils.resize(image, width=700)\n",
    "\th, w = image.shape[0], image.shape[1]\n",
    "\tbi = np.zeros((h,w,3), np.uint8)\n",
    "\tbi.fill(255)\n",
    "\n",
    "ctr = 0\n",
    "while True:\n",
    "\t(grabbed, image) = cap.read()\n",
    "\n",
    "\tif not grabbed:\n",
    "\t\tbreak\n",
    "\tctr+=1\n",
    "\timage = imutils.resize(image, width=700)\n",
    "\th, w = image.shape[0], image.shape[1]\n",
    "\t#print(h,w)\n",
    "\tresults, numbers = pedestrian_detection(image, model, layer_name,\n",
    "\t\tpersonidz=LABELS.index(\"person\"))\n",
    "\t\n",
    "\tcentroids_of_this_frame = []*0\n",
    "\tareas_of_bounding_boxes_of_this_frame = {}\n",
    "\tfor res in results:\n",
    "\t\tcentroids_of_this_frame.append(res[2])\n",
    "\t\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\tx, y\t\t\t\t\t\tx+w, y\t\t\t\t\tx, y+h\t\t\t\t\tx+w, y+h\n",
    "\t\tareas_of_bounding_boxes_of_this_frame.update({res[2]:((res[1][0], res[1][1]), (res[1][3], res[1][0]), (res[1][0], res[1][3]), (res[1][2], res[1][3]))})\n",
    "\t\n",
    "\tabbtf = sorted(areas_of_bounding_boxes_of_this_frame.items(), key = lambda kv:(kv[1], kv[0]))\n",
    "\n",
    "\tprint(abbtf)\n",
    "\t\n",
    "\t'''\n",
    "\tfor i in range(len(abbtf)):\n",
    "\t\tif(i<=len(abbtf)):\n",
    "\t\t\tabbtf[i] = list(abbtf[i])\n",
    "\t\t\tabbtf[i].append(\"\")\n",
    "\n",
    "\tinput()\n",
    "\t'''\n",
    "\t#neigh = {\"orange\":[], \"green\":[], \"red\":[]}\n",
    "\tneigh = {}\n",
    "\tvote = {}\n",
    "\tvote_status = {}\n",
    "\tfor i in centroids_of_this_frame:\n",
    "\t\tvote.update({i:[]*0})\n",
    "\t\n",
    "\t# Code for Area of overlap\n",
    "\t# Take any two boxes, calculate euclidean distance between (X,Y) (x+w,y)\n",
    "\n",
    "\tfor i in range(len(centroids_of_this_frame)-1):\n",
    "\t\t#Box_prime = areas_of_bounding_boxes_of_this_frame[centroids_of_this_frame[i]]\n",
    "\t\tBox_prime = centroids_of_this_frame[i]\n",
    "\t\t#neigh.update({centroids_of_this_frame[i]:[]*0})\n",
    "\t\tprint(\"Box Prime  :\", Box_prime)\n",
    "\t\tfor j in range(1, len(centroids_of_this_frame)):\n",
    "\t\t\t#Box_sec = areas_of_bounding_boxes_of_this_frame[centroids_of_this_frame[j]]\n",
    "\t\t\tBox_sec = centroids_of_this_frame[j]\n",
    "\t\t\tprint(\"Box Secondary  :\", Box_sec)\n",
    "\t\t\n",
    "\t\t\t#if(sp.distance.euclidean(Box_prime[1], Box_sec[0])<=0):\n",
    "\t\t\tif(sp.distance.euclidean(Box_prime[1], Box_sec[0])<=50.0):\n",
    "\n",
    "\t\t\t\t#neigh[centroids_of_this_frame[i]].append([centroids_of_this_frame[j], \"red\"])\n",
    "\t\t\t\tneigh[centroids_of_this_frame[i]] = neigh[centroids_of_this_frame[j]] = \"red\"\n",
    "\n",
    "\t\t\t#elif(0 < sp.distance.euclidean(Box_prime[1], Box_sec[0]) <= 25):\n",
    "\t\t\telif(50.0 < sp.distance.euclidean(Box_prime[1], Box_sec[0])<=300.0):\n",
    "\n",
    "\t\t\t\t#neigh[centroids_of_this_frame[i]].append([centroids_of_this_frame[j], \"orange\"])\n",
    "\t\t\t\tneigh[centroids_of_this_frame[i]] = neigh[centroids_of_this_frame[j]] = \"orange\"\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\t\n",
    "\t\t\t\t#neigh[centroids_of_this_frame[i]].append([centroids_of_this_frame[j], \"green\"])\n",
    "\t\t\t\tneigh[centroids_of_this_frame[i]]  = neigh[centroids_of_this_frame[j]] = \"green\"\n",
    "\t#ctr = 0\n",
    "\tfor res in results:\n",
    "\t\tprint(res[2])\n",
    "\t\t\n",
    "\t\tif (ctr%10==0):\n",
    "\t\t\tcv2.circle(bi, res[2], 1, (255, 34, 38), 3)\n",
    "\t\telse:\n",
    "\t\t\tpass\n",
    "\t\tif (neigh[res[2]]):\n",
    "\t\t\t#print(neigh[res[2]][0][1])\n",
    "\t\t\tif(neigh[res[2]] == \"orange\"):\n",
    "\t\t\t\t# going away : script kiddie version\n",
    "\t\t\t\t#if (h*w) - ((res[1][2]-res[1][0])*(res[1][3]res[1][1]))<100:\n",
    "\t\t\t\tif ((res[1][2]-res[1][0])*(res[1][3]-res[1][1]))<100:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 165, 255), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"D\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Depart\n",
    "\t\t\t\t#coming towards : script kiddie version\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 165, 255), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"A\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Approach\n",
    "\t\t\t\n",
    "\t\t\telif(neigh[res[2]] == \"red\"):\n",
    "\t\t\t\t# going away : script kiddie version\n",
    "\t\t\t\t#if (h*w) - ((res[1][2]-res[1][0])*(res[1][3]res[1][1]))<100:\n",
    "\t\t\t\tif ((res[1][2]-res[1][0])*(res[1][3]-res[1][1]))<100:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 0, 255), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"D\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Depart\n",
    "\t\t\t\t#coming towards : script kiddie version\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 0, 255), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"A\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Approach\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\t# going away : script kiddie version\n",
    "\t\t\t\t#if (h*w) - ((res[1][2]-res[1][0])*(res[1][3]res[1][1]))<100:\n",
    "\t\t\t\tif ((res[1][2]-res[1][0])*(res[1][3]-res[1][1]))<100:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 255, 0), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"D\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Depart\n",
    "\t\t\t\t#coming towards : script kiddie version\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 255, 0), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"A\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Approach\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\n",
    "\t\t\tif ((res[1][2]-res[1][0])*(res[1][3]-res[1][1]))<100:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 255, 0), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"D\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Depart\n",
    "\t\t\t\t#coming towards : script kiddie version\n",
    "\t\t\telse:\n",
    "\t\t\t\t\tcv2.rectangle(image, (res[1][0],res[1][1]), (res[1][2],res[1][3]), (0, 255, 0), 2)\n",
    "\t\t\t\t\tcv2.putText(image, str(res[-1])+\"A\", (res[1][0],res[1][1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2) #Approach\n",
    "\t\t\t\t\t \n",
    "\t\t\n",
    "\t# No. Of Frames\n",
    "\tcv2.putText(image, str(numbers), (20,20), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 4)\n",
    "\timage = cv2.bitwise_and(bi, image)\n",
    "\tcv2.imshow(\"Detection\",image)\n",
    "\tcv2.imshow(\"Tracking\",bi)\n",
    "\t\n",
    "\tf_c += 1\n",
    "\tif(f_c%10==0):\n",
    "\t\tcv2.imwrite(\"/home/arima/Downloads/yolo_pres/frames_op_with_cnt/frame_{}.jpg\".format(f_c), image)\n",
    "\n",
    "\tkey = cv2.waitKey(1)\n",
    "\tif key == 27:\n",
    "\t\tbreak\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
